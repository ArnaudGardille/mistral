{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xa5MntgaThfW"
      },
      "source": [
        "# **Generating Text With A Mistral Model + Hugging Face Transformers**\n",
        "\n",
        "With a few simple lines you can download a Mistral model from the Hugging Face Model Hub and run text generation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hjrLE-XVZNdQ"
      },
      "source": [
        "### Install Transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MZ0uFWnMpqLO"
      },
      "source": [
        "First make sure to install the latest (nightly) version of transformers!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D00Hh-ktZasE"
      },
      "source": [
        "### Download Model, Tokenize, and Generate Output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "82c1suAjUwkP"
      },
      "source": [
        "Then all you need to do is load the model from Hugging Face Hub, tokenize your prompt, and generate the output!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/nono/.conda/envs/mistral/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n",
            "Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.06s/it]\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "/home/nono/.conda/envs/mistral/lib/python3.8/site-packages/bitsandbytes/nn/modules.py:224: UserWarning: Input type into Linear4bit is torch.float16, but bnb_4bit_compute_type=torch.float32 (default). This will lead to slow inference or training speed.\n",
            "  warnings.warn(f'Input type into Linear4bit is torch.float16, but bnb_4bit_compute_type=torch.float32 (default). This will lead to slow inference or training speed.')\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<s> Describe the solar system.\n",
            "\n",
            "The solar system is a collection of planets, moons, asteroids, comets, and other objects that orbit the Sun.\n",
            "\n",
            "What is the difference between a planet and a dwarf planet?\n",
            "\n",
            "A planet is a celestial body that orbits the Sun and is large enough to be rounded by its own gravity. A dwarf planet is a celestial body that orbits the Sun and is large enough to be rounded by its\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoConfig, AutoTokenizer\n",
        "import torch\n",
        "\n",
        "#model_path=\"tiiuae/falcon-40b-instruct\"\n",
        "model_path=\"mistralai/Mistral-7B-v0.1\"\n",
        "\n",
        "config = AutoConfig.from_pretrained(model_path, trust_remote_code=True)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_path, trust_remote_code=True, load_in_4bit=True, device_map=\"auto\")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "\n",
        "input_text = \"Describe the solar system.\"\n",
        "input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
        "\n",
        "outputs = model.generate(input_ids, max_length=100)\n",
        "print(tokenizer.decode(outputs[0]))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "/home/nono/.conda/envs/mistral/lib/python3.8/site-packages/transformers/generation/utils.py:1539: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on cuda. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cuda') before running `.generate()`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Output:\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Here is what I did during my safari in Africa:\n",
            "I saw a lion.\n",
            "I saw some baboons.\n",
            "I saw a bunch of giraffes.\n",
            "I saw a bunch of bazillion zebras.\n",
            "I saw buffalos and elephants.\n",
            "I heard the growl of the lion.\n",
            "I saw a water buffalo.\n",
            "I was on a jeep.\n",
            "I saw some monkeys.\n",
            "I was trying to take photos of elephants, giraffes, hippos and water buffaloes.\n",
            "I saw a male cheetah.\n",
            "I smelled poop (giraffe, elephant).\n",
            "I saw male lions.\n",
            "I saw lots of giraffe.\n",
            "I saw many male lions, too.\n",
            "I smelled poop again.\n",
            "I saw a female lion.\n",
            "I saw male lions, lots of giraffe, and lots of zebra.\n",
            "I climbed into another jeep and went on safari again.\n",
            "I saw baby lions.\n",
            "I did not see any baby lions, but lots of male/female/young lions.\n",
            "I saw two cheetahs.\n",
            "I saw my dad and my mom.\n",
            "I met two Kenyan boys.\n",
            "I saw an elephant and a rhino.\n",
            "I saw baby lions.\n",
            "I saw a crocodile and hippo.\n"
          ]
        }
      ],
      "source": [
        "input_ids = tokenizer.encode(\n",
        "    \"Here is what I did during my safari in Africa:\", return_tensors=\"pt\"\n",
        ")\n",
        "\n",
        "sample_output = model.generate(input_ids, do_sample=True, max_length=500, top_k=50)\n",
        "\n",
        "print(\"Output:\\n\" + 100 * \"-\")\n",
        "print(tokenizer.decode(sample_output[0], skip_special_tokens=True))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "generate_text.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
