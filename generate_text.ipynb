{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xa5MntgaThfW"
      },
      "source": [
        "# **Generating Text With A Mistral Model + Hugging Face Transformers**\n",
        "\n",
        "With a few simple lines you can download a Mistral model from the Hugging Face Model Hub and run text generation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hjrLE-XVZNdQ"
      },
      "source": [
        "### Install Transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MZ0uFWnMpqLO"
      },
      "source": [
        "First make sure to install the latest (nightly) version of transformers!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j9ZsP2MzT3HE",
        "outputId": "02735734-35c5-4d51-be82-c98773a672cf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/huggingface/transformers.git\n",
            "Successfully installed huggingface-hub-0.0.19 pyyaml-5.4.1 sacremoses-0.0.46 tokenizers-0.10.3 transformers-4.12.0.dev0\n"
          ]
        }
      ],
      "source": [
        "!pip install git+https://github.com/huggingface/transformers.git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D00Hh-ktZasE"
      },
      "source": [
        "### Download Model, Tokenize, and Generate Output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "82c1suAjUwkP"
      },
      "source": [
        "Then all you need to do is load the model from Hugging Face Hub, tokenize your prompt, and generate the output!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 183,
          "referenced_widgets": [
            "a6801941b47f4433a8518de3d8d2afc8",
            "e05047fde653479b906b4e457a38966b"
          ]
        },
        "id": "mVbU3jRWU3kZ",
        "outputId": "c519e387-4e4c-4537-f80c-47e3e8acc2b6"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a6801941b47f4433a8518de3d8d2afc8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/0.98k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e05047fde653479b906b4e457a38966b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/701M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Output:\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Hello my name is Alex Rauch. Iâ€™m an English professor, I teach at the University of Oxford and I write about a lot of controversial topics throughout my blog.\n",
            "\n",
            "You read that correctly. I teach about a lot of\n"
          ]
        }
      ],
      "source": [
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "\n",
        "model = GPT2LMHeadModel.from_pretrained(\"stanford-crfm/beren-x49-checkpoint-400000\")\n",
        "\n",
        "input_ids = tokenizer.encode(\n",
        "    \"Hello my name is\", return_tensors=\"pt\"\n",
        ")\n",
        "\n",
        "sample_output = model.generate(input_ids, do_sample=True, max_length=50, top_k=50)\n",
        "\n",
        "print(\"Output:\\n\" + 100 * \"-\")\n",
        "print(tokenizer.decode(sample_output[0], skip_special_tokens=True))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "generate_text.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
